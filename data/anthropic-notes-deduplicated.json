[
  {
    "title": "Making a Request",
    "content": "Making API Request to Anthropic = Process involving 4 setup steps and understanding message structure\n\nSetup Steps:\n1. Install packages = pip install anthropic python-dotenv in Jupyter notebook\n2. Store API key = Create .env file with ANTHROPIC_API_KEY=\"your_key\" (ignore in version control)\n3. Load environment variable = Use python-dotenv to securely load API key\n4. Create client = Initialize anthropic client and define model variable (claude-3-sonnet)\n\nAPI Request Structure:\n- Function = client.messages.create()\n- Required arguments = model, max_tokens, messages\n- Model = Name of Claude model to use\n- Max_tokens = Safety limit for generation length (not target length)\n- Messages = List containing conversation exchanges\n\nMessage Types:\n- User message = {\"role\": \"user\", \"content\": \"your text\"} (human-authored content)\n- Assistant message = Contains model-generated responses\n\nResponse Access:\n- Full response = Contains metadata and nested structure\n- Text only = message.content[0].text extracts just generated text\n\nExample request structure: client.messages.create(model=model, max_tokens=1000, messages=[{\"role\": \"user\", \"content\": \"What is quantum computing?\"}])",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Prompt Engineering",
    "content": "Prompt Engineering = improving prompts to get more reliable, higher-quality outputs from language models.\n\nModule Structure: Start with initial poor prompt → Apply prompt engineering techniques step-by-step → Evaluate improvements after each technique → Observe performance gains over time.\n\nExample Goal: Generate one-day meal plan for athletes based on height, weight, physical goal, dietary restrictions.\n\nTechnical Setup:\n- Updated eval pipeline with flexible prompt evaluator class\n- Supports concurrency (adjust max_concurrent_tasks based on rate limits)\n- generate_dataset() method creates test cases with specified inputs\n- run_prompt() function processes each test case individually\n\nKey Components:\n- prompt_input_spec = dictionary defining required prompt inputs\n- extra_criteria = additional validation requirements for model grading\n- output.html = formatted evaluation report showing test case results and scores\n\nProcess: Write initial prompt → Interpolate test case inputs → Run evaluation → Apply engineering techniques → Re-evaluate → Repeat until satisfactory performance.\n\nInitial Results: Expect poor scores (example: 2.32) with basic prompts, especially when using less capable models. Scores improve as techniques are applied.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Tool Functions",
    "content": "Tool Functions = Python functions executed automatically when Claude needs extra information to help users.\n\nKey characteristics:\n- Plain Python functions called by Claude when it determines additional data is needed\n- Must use descriptive function names and argument names\n- Should validate inputs and raise errors with meaningful messages\n- Error messages are visible to Claude, allowing it to retry with corrected parameters\n\nBest practices:\n1. Well-named functions and arguments\n2. Input validation with immediate error raising for invalid inputs\n3. Meaningful error messages that guide correction\n\nExample implementation pattern:\n\\`\\`\\`\ndef get_current_datetime(date_format=\"%Y%m%d %H:%M:%S\"):\n    if not date_format:\n        raise ValueError(\"date format cannot be empty\")\n    return datetime.now().strftime(date_format)\n\\`\\`\\`\n\nTool function workflow: Claude identifies need for information → calls tool function → receives result or error → may retry with corrections if error occurred.\n\nPurpose: Extend Claude's capabilities beyond its training data by providing access to real-time information like current datetime, weather, etc.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Using Multiple Tools",
    "content": "Multiple Tools Implementation = Adding additional tools to an existing tool system after initial framework setup.\n\nProcess = 3 steps: (1) Add tool schemas to RunConversation function's tools list, (2) Add conditional cases in RunTool function to handle new tool names, (3) Implement actual tool functions.\n\nKey Components:\n- RunConversation function = Contains tools list that makes Claude aware of available tools\n- RunTool function = Routes tool calls to appropriate functions based on tool name\n- Tool schemas = Define tool structure for the AI model\n- Tool functions = Actual implementation code\n\nExample Tools Added:\n- AddDurationToDateTime = Calculates date/time with duration offset\n- SetReminder = Creates reminder (mock implementation that prints confirmation)\n\nTool Chaining = AI can use multiple tools sequentially in single conversation (e.g., calculate date first, then set reminder with result).\n\nMessage Structure = Assistant responses can contain multiple blocks: text blocks + tool use blocks in same message.\n\nScalability = After initial framework setup, adding new tools becomes simple pattern of schema + routing + implementation.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Agents and Workflows",
    "content": "Workflows and agents = strategies for handling user tasks that can't be completed by Claude in a single request.\n\nDecision rule: Use workflows when you have precise task understanding and know exact steps sequence. Use agents when task details are unclear.\n\nWorkflow = series of calls to Claude for specific problems where steps are predetermined.\n\nExample workflow: Image to 3D model converter\n- Step 1: Claude describes uploaded image in detail\n- Step 2: Claude uses CADQuery Python library to model object from description\n- Step 3: Create rendering of model\n- Step 4: Claude compares rendering to original image\n- Step 5: If inaccurate, repeat from step 2 with feedback\n\nThis follows evaluator-optimizer pattern:\n- Producer = generates output (Claude + CADQuery modeling)\n- Evaluator = assesses output quality (comparison step)\n- Loop continues until evaluator accepts output\n\nKey point: Workflows are implementation patterns that other engineers have successfully used. Identifying workflow patterns doesn't automatically implement them - you still need to write the actual code.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "The Web Search Tool",
    "content": "Web Search Tool = built-in Claude tool for searching web to find up-to-date/specialized information for user questions\n\nImplementation = no custom code needed, Claude handles search execution automatically\n\nSchema Requirements:\n- type: \"web_search_20250305\"  \n- name: \"web_search\"\n- max_uses: number (limits total searches, default 5)\n- allowed_domains: optional list to restrict search to specific domains\n\nResponse Structure:\n- Text blocks = Claude's explanatory text\n- Tool use blocks = search queries Claude executed  \n- Web search result blocks = found pages (title, URL)\n- Citation blocks = specific text supporting Claude's statements\n\nKey Features:\n- Multiple searches possible per request (up to max_uses limit)\n- Domain restriction available for quality control\n- Citation system links statements to source material\n\nUI Rendering Pattern:\n- Display text blocks as normal text\n- Show search results as reference list\n- Highlight citations with source attribution (domain, title, URL, quoted text)\n\nUse Case Example: Restricting to NIH.gov for medical/exercise advice ensures scientifically-backed information vs generic web content.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "A Multi-Index Rag Pipeline",
    "content": "Multi-Index RAG Pipeline = system combining semantic search (vector index) and lexical search (BM25 index) for improved retrieval accuracy.\n\nKey Components:\n- Vector Index = semantic similarity search using embeddings\n- BM25 Index = lexical/keyword-based search \n- Retriever Class = wrapper that forwards queries to both indexes and merges results\n\nReciprocal Rank Fusion = technique for merging search results from different indexes. Formula: RRF_score = sum of (1/(rank + 1)) across all search methods for each document. Documents ranked by highest combined score.\n\nExample: Vector search returns [doc2, doc7, doc6], BM25 returns [doc6, doc2, doc7]. After RRF calculation, final ranking becomes [doc2, doc6, doc7] because doc2 ranked high in both methods.\n\nBenefits:\n- Improved search accuracy by combining different search paradigms\n- Modular design with standardized API (search() and add_document() methods)\n- Easy to extend with additional search indexes\n- Better handling of edge cases where single method fails\n\nImplementation pattern allows multiple search methodologies to work together while maintaining separate, isolated index classes.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "MCP Clients",
    "content": "MCP Client = communication interface between your server and MCP server, provides access to server's tools\n\nTransport agnostic = client/server can communicate via multiple protocols (stdio, HTTP, WebSockets)\n\nCommon setup = client and server on same machine using standard input/output\n\nCommunication = message exchange defined by MCP spec\n\nKey message types:\n- list tools request = client asks server for available tools\n- list tools result = server responds with tool list  \n- call tool request = client asks server to run tool with arguments\n- call tool result = server responds with tool execution result\n\nTypical flow:\n1. User queries server\n2. Server requests tool list from MCP client\n3. MCP client sends list tools request to MCP server\n4. MCP server responds with list tools result\n5. Server sends query + tools to Claude\n6. Claude requests tool execution\n7. Server asks MCP client to run tool\n8. MCP client sends call tool request to MCP server\n9. MCP server executes tool (e.g. GitHub API call)\n10. Results flow back through chain: MCP server → MCP client → server → Claude → user\n\nPurpose = enables servers to delegate tool execution to specialized MCP servers while maintaining Claude integration",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Accessing the API",
    "content": "API Access Flow = 5-step process from user input to response display\n\nStep 1: Client sends user text to developer's server (never access Anthropic API directly from client apps to keep API key secret)\n\nStep 2: Server makes request to Anthropic API using SDK (Python, TypeScript, JavaScript, Go, Ruby) or plain HTTP. Required parameters = API key + model name + messages list + max_tokens limit\n\nStep 3: Text generation process has 4 stages:\n- Tokenization = breaking input into tokens (words/word parts/symbols/spaces)\n- Embedding = converting tokens to number lists representing all possible word meanings\n- Contextualization = adjusting embeddings based on neighboring tokens to determine precise meaning\n- Generation = output layer produces probabilities for next word, model selects using probability + randomness, adds selected word, repeats process\n\nStep 4: Model stops when max_tokens reached or special end_of_sequence token generated\n\nStep 5: API returns response with generated text + usage counts + stop_reason to server, server sends to client for display\n\nToken = text chunk (word/part/symbol)\nEmbedding = numerical representation of word meanings\nContextualization = meaning refinement using neighboring words\nMax_tokens = generation length limit\nStop_reason = why model stopped generating",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Multi-Turn Conversations with Tools",
    "content": "Multi-Turn Tool Conversations = conversations where Claude uses multiple tools sequentially to answer a single user query.\n\nTool Chaining Process = user asks question → Claude requests first tool → tool executed → result returned → Claude requests second tool → tool executed → result returned → Claude provides final answer.\n\nExample Flow = user asks \"what day is 103 days from today\" → Claude calls get_current_datetime → Claude calls add_duration_to_datetime → Claude provides answer.\n\nImplementation Pattern = while loop that continues calling Claude until no more tool requests, checking each response for tool_use blocks.\n\nrun_conversation Function = takes initial messages, loops through Claude calls, executes requested tools, adds results to conversation, continues until final response.\n\nRequired Refactors:\n- add_user_message/add_assistant_message = updated to handle multiple message blocks instead of just plain text\n- chat function = accepts tools parameter, returns entire message instead of just first text block\n- text_from_message helper = extracts all text blocks from a message with multiple content blocks\n\nKey Insight = can't predict how many tools user queries will require, so system must handle arbitrary chains of tool calls automatically.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Temperature",
    "content": "Temperature = parameter (0-1) that controls randomness in Claude's text generation by influencing token selection probabilities.\n\nText generation process: Input text → tokenization → probability assignment to possible next tokens → token selection based on probabilities → repeat.\n\nTemperature effects:\n- Temperature 0 = deterministic output, always selects highest probability token\n- Higher temperature = increases chance of selecting lower probability tokens, more creative/unexpected outputs\n\nUsage guidelines:\n- Low temperature (near 0) = data extraction, factual tasks requiring consistency\n- High temperature (near 1) = creative tasks like brainstorming, writing, jokes, marketing\n\nImplementation: Add temperature parameter to model API calls. Higher values don't guarantee different outputs, just increase probability of variation.\n\nKey insight: Temperature directly manipulates the probability distribution of next token selection, making high-probability tokens more/less dominant in the selection process.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Tools for Structured Data",
    "content": "Tools for Structured Data = alternative method to extract structured JSON from data sources using Claude's tool system instead of message pre-fill and stop sequences.\n\nKey differences from prompt-based extraction:\n- More reliable output\n- More complex setup\n- Requires JSON schema specification\n\nCore Process:\n1. Define JSON schema for tool where inputs = desired data structure\n2. Send prompt + schema to Claude\n3. Claude calls tool with structured arguments matching schema\n4. Extract JSON from tool use block (no tool result needed)\n\nCritical requirement = Force tool calling using tool_choice parameter:\n- tool_choice = {\"type\": \"tool\", \"name\": \"your_tool_name\"}\n- Ensures Claude always calls specified tool\n\nImplementation steps:\n1. Create schema definition for extraction tool\n2. Update chat function to accept tool_choice parameter\n3. Pass tool_choice to client.messages.create()\n4. Access structured data from response.content[0].input\n\nUse cases = When reliability more important than simplicity. Prompt-based methods better for quick/simple extractions, tools better for complex/reliable extractions.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Implementing Multiple Turns",
    "content": "**Multiple Turns Implementation = continuously calling Claude until it stops requesting tools**\n\n**Stop Reason Field = indicates why Claude stopped generating text**\n- stop_reason = \"tool_use\" means Claude wants to call a tool\n- Other values exist but tool_use is most commonly checked\n\n**run_conversation Function = main loop that:**\n1. Calls Claude with messages + available tools\n2. Adds assistant response to conversation history\n3. Checks stop_reason - if not \"tool_use\", breaks loop\n4. If tool_use, calls run_tools function\n5. Adds tool results as user message\n6. Repeats until no more tool requests\n\n**run_tools Function = processes multiple tool use blocks:**\n1. Filters message.content for blocks with type=\"tool_use\"\n2. Iterates through each tool request\n3. Runs appropriate tool function via run_tool helper\n4. Creates tool_result blocks with: type=\"tool_result\", tool_use_id=original_id, content=JSON_encoded_output, is_error=boolean\n5. Returns list of all tool result blocks\n\n**run_tool Function = dispatcher that:**\n- Takes tool_name and tool_input\n- Uses if statements to match tool names to functions\n- Executes appropriate tool function\n- Scalable for adding multiple tools\n\n**Error Handling = try/except blocks around tool execution:**\n- Success: is_error=false, content=tool_output\n- Failure: is_error=true, content=error_message\n\n**Key Architecture Points:**\n- Assistant messages can contain multiple blocks (text + multiple tool_use)\n- Each tool_use block gets separate tool_result response\n- Tool results sent back as user message containing all results\n- Process repeats until Claude provides final text-only response",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "The Text Edit Tool",
    "content": "Text Editor Tool = built-in Claude tool for file/text operations (read, write, create, replace, undo files/directories)\n\nKey characteristics:\n- Only JSON schema built into Claude, implementation must be custom-coded\n- Schema stub sent to Claude gets auto-expanded to full schema\n- Schema type string varies by Claude model version (3.5 vs 3.7 have different dates)\n- Enables Claude to act as software engineer out-of-the-box\n\nRequired implementation:\n- Custom class/functions to handle Claude's tool use requests\n- Functions for: view files, string replace, create files, etc.\n- Actual file system operations not provided by Claude\n\nWorkflow:\n1. Send minimal schema stub to Claude (name + type with version-specific date)\n2. Claude expands to full schema internally\n3. Claude sends tool use requests\n4. Custom implementation executes actual file operations\n5. Results sent back to Claude\n\nUse cases:\n- Replicate AI code editor functionality\n- File system operations where native editors unavailable\n- Automated code generation/refactoring\n- Multi-file project manipulation\n\nBenefits = approximates fancy code editor capabilities through API calls rather than GUI interaction.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Text Chunking Strategies",
    "content": "Text Chunking Strategies = process of dividing documents into smaller pieces for RAG pipelines\n\nCore Problem: Chunking quality directly impacts RAG performance. Poor chunking leads to irrelevant context retrieval (e.g., medical \"bug\" text retrieved for software engineering query about bugs).\n\nThree Main Strategies:\n\n1. Size-Based Chunking = dividing text into equal-length strings\n- Pros: Easy to implement, most common in production\n- Cons: Cut-off words, lacks context\n- Solution: Overlap strategy = include characters from neighboring chunks to preserve context\n- Trade-off: Creates text duplication but improves chunk meaning\n\n2. Structure-Based Chunking = dividing based on document structure (headers, paragraphs, sections)\n- Best for structured documents (markdown, HTML)\n- Limitation: Requires guaranteed document formatting\n- Example: Split on markdown headers (##) to create section-based chunks\n\n3. Semantic-Based Chunking = using NLP to group related sentences/sections\n- Most advanced technique\n- Groups consecutive sentences based on semantic similarity\n- Complex implementation\n\nKey Implementation Notes:\n- Chunk by character = most reliable fallback, works with any document type\n- Chunk by sentence = good middle ground if sentence detection works reliably\n- Chunk by section = optimal results but requires structured input\n- Strategy choice depends on document type guarantees and use case requirements\n\nRule: No universal best chunking method - depends on document structure guarantees and specific use case.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Implementing the Rag Flow",
    "content": "RAG Flow Implementation = practical walkthrough of 5-step retrieval-augmented generation process\n\nStep 1: Text Chunking = split document into sections using chunk_by_section function on report.MD file\n\nStep 2: Embedding Generation = create vector representations for each chunk using generate_embedding function (supports single string or list of strings input)\n\nStep 3: Vector Store Population = create vector index instance, loop through chunk-embedding pairs using zip(), store each pair with store.add_vector(embedding, {content: chunk}). Store original text with embeddings for meaningful retrieval results.\n\nStep 4: Query Processing = user asks question \"what did software engineering department do last year\", generate embedding for user query\n\nStep 5: Similarity Search = use store.search(user_embedding, 2) to find 2 most relevant chunks, returns results with cosine distances (0.71 for section two, 0.72 for methodology section)\n\nKey Components:\n- Vector Index Class = custom vector database implementation\n- Cosine Distance = similarity metric between query and stored embeddings\n- Metadata Storage = storing original text content alongside embeddings enables meaningful retrieval\n\nWorkflow complete but has limitations requiring further improvements.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Contextual Retrieval",
    "content": "Contextual Retrieval = technique to improve RAG pipeline accuracy by adding context to document chunks before embedding.\n\nProblem: When documents are split into chunks, individual chunks lose context from the original document, reducing retrieval accuracy.\n\nSolution: Pre-processing step that adds contextual information to each chunk before inserting into retriever database.\n\nProcess:\n1. Take individual chunk + original source document\n2. Send to LLM (Claude) with prompt asking to generate situating context\n3. LLM generates brief context explaining chunk's relationship to larger document\n4. Join generated context with original chunk = \"contextualized chunk\"\n5. Use contextualized chunk as input to vector/BM25 indexes\n\nLarge Document Handling: If source document too large for single prompt, use selective context strategy:\n- Include starter chunks (1-3) from document beginning for summary/abstract\n- Include chunks immediately before target chunk for local context\n- Skip middle chunks that provide less relevant context\n\nImplementation: add_context function takes text chunk + source text, generates context via LLM, concatenates context with original chunk, returns contextualized version.\n\nBenefit: Chunks retain ties to larger document structure and cross-references, improving retrieval accuracy for complex documents with interconnected sections.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Extended Thinking",
    "content": "Extended Thinking = Claude feature that allows reasoning time before generating final response\n\nKey mechanics:\n- Displays separate thinking process visible to users\n- Increases accuracy for complex tasks but adds cost (charged for thinking tokens) and latency\n- Thinking budget = minimum 1024 tokens allocated for thinking phase\n- Max tokens must exceed thinking budget (e.g., budget 1024 requires max_tokens ≥ 1025)\n\nWhen to use:\n- Enable after prompt optimization fails to achieve desired accuracy\n- Use prompt evals to determine necessity\n\nResponse structure:\n- Thinking block = contains reasoning text + cryptographic signature\n- Text block = final response\n- Signature = prevents tampering with thinking text (safety measure)\n\nSpecial cases:\n- Redacted thinking blocks = encrypted thinking text flagged by safety systems\n- Provided for conversation continuity without losing context\n- Can force redacted blocks using test string: \"entropic magic string triggered redacted thinking [special characters]\"\n\nImplementation:\n- Set thinking=true and thinking_budget parameter\n- Ensure max_tokens > thinking_budget for adequate response generation capacity",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Image Support",
    "content": "Claude Vision Capabilities = ability to process images within user messages for analysis, comparison, counting, and description tasks.\n\nImage Limitations:\n- Max 100 images per request\n- Size/dimension restrictions apply\n- Images consume tokens (charged based on pixel height/width calculation)\n\nImage Block Structure = special block type within user messages that holds either raw image data (base64) or URL reference to online image. Multiple image blocks allowed per message.\n\nCritical Success Factor = strong prompting techniques required for accurate results. Simple prompts often fail.\n\nPrompting Techniques for Images:\n- Step-by-step analysis instructions\n- One-shot/multi-shot examples (alternating image and text pairs)\n- Clear guidelines and verification steps\n- Structured analysis frameworks\n\nExample Use Case = automated fire risk assessment from satellite imagery analyzing tree density, property access, roof overhang, and assigning numerical risk scores.\n\nImplementation = base64 encode image data, create message with image block (type: image, source: base64, media_type, data) followed by text block containing detailed prompt instructions.\n\nKey Takeaway = image accuracy depends entirely on prompt sophistication, not just image quality.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Prompts in the Client",
    "content": "MCP Client Prompt Implementation:\n\nList prompts = await self.session.list_prompts(), return result.prompts\nGet prompt = await self.session.get_prompt(prompt_name, arguments), return result.messages\n\nPrompt workflow:\n1. Define prompt in MCP server with expected arguments (e.g., document_id)\n2. Client calls get_prompt with prompt name + arguments dictionary\n3. Arguments passed as keyword arguments to prompt function\n4. Function interpolates arguments into prompt text\n5. Returns messages array for direct feeding to LLM\n\nKey concept: Prompts are server-defined templates that clients can invoke with specific arguments to generate contextualized instructions for LLMs. Arguments flow from client call → prompt function → interpolated prompt text → LLM consumption.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Claude Code in Action",
    "content": "Claude Code = AI coding assistant that functions as a collaborative engineer on projects, not just a code generator.\n\nKey capabilities: project setup, feature design, code writing, testing, deployment, error fixing in production.\n\nSetup workflow:\n- Download project, open in editor\n- Run \\`claude\\` command to launch\n- Ask Claude to read README and execute setup directions\n- Run \\`init\\` command = Claude scans codebase for architecture/coding style, creates claude.md file\n- claude.md = automatically included context for future requests\n\nMemory types: Project (shared), Local, User memory files.\n\nContext management:\n- Use # symbol to add specific notes to memory\n- Can manually edit claude.md or rerun init to update\n- Claude can handle Git operations (staging, committing)\n\nEffective prompting strategies:\n\nMethod 1 - Three-step workflow:\n1. Identify relevant files, ask Claude to analyze them\n2. Describe feature, ask Claude to plan solution (no code yet)\n3. Ask Claude to implement the plan\n\nMethod 2 - Test-driven development:\n1. Provide relevant context\n2. Ask Claude to suggest tests for the feature\n3. Select and implement chosen tests\n4. Ask Claude to write code until tests pass\n\nCore principle: Claude Code = effort multiplier. More detailed instructions = significantly better results. Treat as collaborative engineer, not just code generator.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Enhancements with MCP Servers",
    "content": "Claude Code = AI assistant with embedded MCP (Model Context Protocol) client that can connect to MCP servers to expand functionality.\n\nMCP Server Integration = Connect external tools/services to Claude Code via command: \\`claude mcp add [server-name] [startup-command]\\`\n\nExample Implementation = Document processing server exposing \"Document Path to Markdown\" tool, allowing Claude Code to read PDF/Word documents by running \\`uv run main.py\\`\n\nDynamic Capability Expansion = MCP servers add new functions to Claude Code in real-time without core modifications.\n\nCommon Use Cases = Production monitoring (Sentry), project management (Jira), communication (Slack), custom development workflow tools.\n\nKey Benefit = Significant flexibility increase for development workflows through modular server connections.\n\nSetup Process = 1) Create MCP server with tools, 2) Add server to Claude Code with name and startup command, 3) Restart Claude Code to access new capabilities.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Automated Debugging",
    "content": "Automated Debugging = using AI (Claude) to automatically detect, analyze, and fix production errors without manual intervention.\n\nCore Workflow:\n1. GitHub Action runs daily to check production environment\n2. Fetches CloudWatch logs from last 24 hours\n3. Claude identifies errors, deduplicates them\n4. Claude analyzes each error and generates fixes\n5. Creates pull request with proposed solutions\n\nKey Components:\n- GitHub Actions for scheduling/automation\n- AWS CLI for log retrieval\n- Claude Code for error analysis and code fixes\n- CloudWatch for production error monitoring\n\nBenefits:\n- Catches production-only errors (issues not present in development)\n- Reduces manual log hunting and debugging time\n- Provides context-aware fixes with explanations\n- Creates reviewable pull requests for changes\n\nCommon Use Case: Configuration errors between environments (invalid model IDs, API keys, etc. that work locally but fail in production)\n\nImplementation Requirements: Repository access, cloud logging service, AI coding assistant, CI/CD pipeline integration.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Routing Workflows",
    "content": "Routing Workflows = workflow pattern that categorizes user input to determine appropriate processing pipeline\n\nKey mechanism: Initial request to Claude categorizes user input into predefined genres/categories. Based on categorization response, system routes to specialized processing pipeline with customized prompts/tools.\n\nExample flow:\n1. User enters topic (e.g., \"Python functions\")\n2. Claude categorizes topic (e.g., \"educational\")\n3. System uses educational-specific prompt template\n4. Claude generates script with educational tone/structure\n\nBenefits: Ensures output matches topic nature. Programming topics get educational treatment with definitions/explanations. Entertainment topics get trendy language/engaging hooks.\n\nStructure: One routing step → Multiple specialized processing pipelines → Each pipeline has customized prompts/tools for specific category\n\nUse case: Social media video script generation where different topics require different tones and approaches.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Controlling Model Output",
    "content": "**Controlling Model Output = Two key techniques beyond prompt modification**\n\n**Pre-filling Assistant Messages = Manually adding assistant message at end of conversation to steer response direction**\n\nHow it works:\n- Assemble messages list with user prompt + manual assistant message\n- Claude sees assistant message as already authored content\n- Claude continues response from exact end of pre-filled text\n- Response gets steered toward pre-filled direction\n\nKey point: Claude continues from exact endpoint of pre-fill, not complete sentences. Must stitch together pre-fill + generated response.\n\nExample: Pre-fill \"Coffee is better because\" → Claude continues with justification for coffee\n\n**Stop Sequences = Force Claude to halt generation when specific string appears**\n\nHow it works:\n- Provide stop sequence string in chat function\n- When Claude generates that exact string, response immediately stops\n- Generated stop sequence text not included in final output\n\nExample: Prompt \"count 1 to 10\" + stop sequence \"five\" → Output stops at \"four, \" (five not included)\n\nRefinement: Stop sequence \", five\" → Clean output \"one, two, three, four\"\n\nBoth techniques provide precise control over response direction and length without changing core prompts.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Running the Eval",
    "content": "Eval execution process = merging test cases with prompts, running through LLM, and grading outputs.\n\nTest case = individual record from dataset (JSON object).\n\nThree core functions:\n- run_prompt = merges test case with prompt, sends to Claude, returns output\n- run_test_case = calls run_prompt, grades result, returns summary dictionary \n- run_eval = loops through dataset, calls run_test_case for each, assembles results\n\nBasic prompt structure = \"Please solve the following task: [test_case_task]\" (v1 starting point).\n\nCurrent limitations = no output formatting instructions, hardcoded scoring (score=10), verbose Claude responses.\n\nRuntime = ~31 seconds with Haiku model for full dataset execution.\n\nOutput format = array of objects containing Claude output, original test case, and score.\n\nNext step = implement proper grading system to replace hardcoded scores.\n\nEval pipeline core = dataset + prompt + LLM + grader, with minimal code complexity.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Being Specific",
    "content": "Being Specific = adding guidelines or steps to direct model output in particular direction\n\nTwo types of guidelines:\nType A (Attributes) = list qualities/attributes desired in output (length, structure, format)\nType B (Steps) = provide specific steps for model to follow in reasoning process\n\nType A controls output characteristics. Type B controls how model arrives at answer.\n\nBoth techniques often combined in professional prompts.\n\nWhen to use:\n- Type A (attributes): recommended for almost all prompts\n- Type B (steps): use for complex problems where you want model to consider broader perspective or additional viewpoints it might not naturally consider\n\nExample improvement: meal planning prompt score jumped from 3.92 to 7.86 when guidelines added, demonstrating significant quality improvement through specificity.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Introducing Tool Use",
    "content": "Tool use = method for Claude to access external information beyond training data.\n\nDefault limitation: Claude only knows information from training data, lacks current/real-time information.\n\nTool use flow:\n1. Send initial request to Claude + instructions for external data access\n2. Claude evaluates if external data needed, requests specific information\n3. Server runs code to fetch requested data from external sources\n4. Send follow-up request to Claude with retrieved data\n5. Claude generates final response using original prompt + external data\n\nWeather example: User asks current weather → Claude requests weather data → Server calls weather API → Claude receives weather data → Claude provides informed weather response.\n\nKey concept: Tools enable Claude to augment responses with live/current information by orchestrating external data retrieval between Claude's requests.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Tool Schemas",
    "content": "Tool Schemas = JSON schema specifications that describe tool functions and their parameters for language models\n\nJSON Schema = data validation specification (not ML-specific) used to validate JSON data, adopted by ML community for tool calling\n\nTool Schema Structure:\n- name: tool identifier \n- description: 3-4 sentences explaining what tool does, when to use, what data it returns\n- input_schema: actual JSON schema describing function arguments with types and descriptions\n\nSchema Generation Trick:\n1. Take tool function to Claude.ai\n2. Prompt: \"write valid JSON schema spec for tool calling for this function, follow best practices in attached documentation\"\n3. Attach Anthropic API documentation tool use page\n4. Copy generated schema\n\nImplementation Pattern:\n- Name functions descriptively\n- Name schemas as [function_name]_schema\n- Import ToolParam from anthropic.types\n- Wrap schema dictionary with ToolParam() to prevent type errors\n\nPurpose = inform Claude about available tools, required arguments, and usage context through standardized JSON validation format",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Handling Message Blocks",
    "content": "**Tool-Enabled Claude Requests**\n\nStep 3: Making requests to Claude with tools = include tool schema in request alongside user message using \\`tools\\` keyword argument containing JSON schema specs.\n\n**Multi-Block Messages**\n\nContent structure change = messages now contain multiple blocks instead of just text blocks.\n\nTool response format = assistant message with:\n- Text block = user-facing explanation \n- Tool use block = contains function name + arguments for tool execution\n\n**Message History Management**\n\nCritical requirement = manually maintain conversation history since Claude stores nothing.\n\nMulti-block handling = append entire response.content (all blocks) to messages list, not just text.\n\nHelper function updates needed = add_user_message and add_assistant_message functions must support multiple blocks instead of single text blocks only.\n\nConversation flow = user message → assistant response with tool use block → execute tool → respond back to Claude with full history.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Text Embeddings",
    "content": "Text Embeddings = numerical representation of text meaning generated by embedding models\n\nEmbedding Model = takes text input, outputs long list of numbers (range -1 to +1)\n\nEmbedding Numbers = scores representing unknown qualities/features of input text. Each number theoretically scores different aspects (happiness, topic relevance, etc.) but actual meaning is unknown to users.\n\nSemantic Search = uses text embeddings to find text chunks related to user questions in RAG pipelines. Solves the search problem of matching user queries to relevant document chunks.\n\nRAG Pipeline Process = extract text chunks → user submits query → find related chunks using semantic search → add relevant chunks as context to prompt\n\nImplementation = Anthropic recommends Voyage AI for embedding generation. Requires separate account/API key. Free to start, easy integration via SDK.\n\nKey Insight = Embeddings enable semantic similarity matching rather than keyword matching, allowing better understanding of text relationships for retrieval tasks.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Introducing MCP",
    "content": "MCP = Model Context Protocol, communication layer providing Claude with context and tools without requiring developers to write tedious code.\n\nArchitecture: MCP client connects to MCP server. Server contains tools, resources, and prompts as internal components.\n\nProblem solved: Eliminates burden of authoring/maintaining numerous tool schemas and functions for service integrations. Example: GitHub chatbot would require implementing tools for repositories, pull requests, issues, projects - significant developer effort.\n\nSolution: MCP server handles tool definition and execution instead of your application server. MCP servers = interfaces to outside services, wrapping functionality into ready-to-use tools.\n\nKey benefits: Developers avoid writing tool schemas and function implementations themselves.\n\nCommon questions:\n- Who creates MCP servers? Anyone, often service providers make official implementations (AWS, etc.)\n- vs direct API calls? MCP eliminates need to author tool schemas/functions yourself\n- vs tool use? MCP and tool use are complementary - MCP handles WHO does the work (server vs developer), both still involve tools\n\nCore value: Shifts integration burden from application developers to MCP server maintainers.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Project Setup",
    "content": "CLI-based chatbot project = teaches MCP client-server interaction through hands-on implementation\n\nProject components:\n- MCP client = connects to custom MCP server\n- MCP server = provides 2 tools (read document, update document)\n- Document collection = fake documents stored in memory only\n\nKey distinction: Normal projects implement either client OR server, not both. This project implements both for educational purposes.\n\nSetup process:\n1. Download CLI_project.zip starter code\n2. Extract and open in code editor\n3. Follow readme.md setup directions\n4. Add API key to .env file\n5. Install dependencies (with/without UV)\n6. Run project: \"uv run main.py\" or \"python main.py\"\n7. Test with chat prompt\n\nExpected outcome = working chat interface that responds to basic queries, ready for MCP feature additions.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Defining Prompts",
    "content": "MCP Prompts = Pre-defined, tested prompt templates that MCP servers expose to client applications for specialized tasks.\n\nPurpose = Instead of users writing ad-hoc prompts, server authors create high-quality, evaluated prompts tailored to their server's domain.\n\nImplementation = Use @mcpserver.prompt decorator with name/description, define function that returns list of messages (user/assistant messages that can be sent directly to Claude).\n\nExample Use Case = Document formatting prompt that takes document ID, instructs Claude to read document using tools, reformat to markdown, and save changes.\n\nKey Benefits = Server-specific expertise, pre-tested quality, reusable across client applications, better results than user-generated prompts.\n\nMessage Structure = Returns base.UserMessage objects containing the formatted prompt text with interpolated parameters.\n\nClient Integration = Prompts appear as autocomplete options (slash commands) in client applications, prompt user for required parameters, then execute the pre-built prompt workflow.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Chaining Workflows",
    "content": "Chaining Workflows = breaking large tasks into series of distinct sequential steps rather than single complex prompt\n\nCore concept: Instead of one massive prompt with multiple requirements, split into separate calls where each focuses on one specific subtask.\n\nExample workflow: User enters topic → search trending topics → Claude selects most interesting → Claude researches topic → Claude writes script → generate video → post to social media\n\nKey benefit: Allows AI to focus on individual tasks rather than juggling multiple constraints simultaneously\n\nPrimary use case: When Claude consistently ignores constraints in complex prompts despite repetition. Common with long prompts containing many \"don't do X\" requirements.\n\nProblem scenario: Long prompt with constraints (don't mention AI, no emojis, professional tone) → Claude violates some constraints regardless of repetition\n\nSolution: Step 1 - Send initial prompt, accept imperfect output. Step 2 - Follow-up prompt asking Claude to rewrite based on specific violations found.\n\nCritical insight: Even simple-seeming workflow becomes essential when dealing with constraint-heavy prompts that AI struggles to follow completely in single pass.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "A Typical Eval Workflow",
    "content": "Typical Eval Workflow = 6-step iterative process for prompt improvement\n\nStep 1: Write initial prompt draft - create baseline prompt to optimize\n\nStep 2: Create evaluation dataset - collection of test inputs (can be 3 examples or thousands, hand-written or LLM-generated)\n\nStep 3: Generate prompt variations - interpolate each dataset input into prompt template\n\nStep 4: Get LLM responses - feed each prompt variation to Claude, collect outputs\n\nStep 5: Grade responses - use grader system to score each response (e.g. 1-10 scale), average scores for overall prompt performance\n\nStep 6: Iterate - modify prompt based on scores, repeat entire process, compare versions\n\nKey points: No standard methodology exists. Many open-source/paid tools available. Can start simple with custom implementation. Grading complexity varies. Objective scoring enables systematic prompt improvement through A/B comparison.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Being Clear and Direct",
    "content": "Being Clear and Direct = Use simple, direct language with action verbs in the first line of prompts to specify the exact task.\n\nFirst line importance = Most critical part of prompt that sets the foundation for AI response.\n\nStructure = Action verb + clear task description + output specifications.\n\nExamples:\n- \"Write three paragraphs about how solar panels work\"\n- \"Identify three countries that use geothermal energy and for each include generation stats\"\n- \"Generate a one day meal plan for an athlete that meets their dietary restrictions\"\n\nKey components = Action verb at start + direct task statement + expected output details.\n\nResult = Improved prompt performance (example showed score increase from 2.32 to 3.92).",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Structure with XML Tags",
    "content": "XML Tags for Prompt Structure = Using XML tags to organize and delineate different content sections within prompts to improve AI comprehension.\n\nPurpose = When interpolating large amounts of content into prompts, XML tags help AI models distinguish between different types of information and understand text grouping.\n\nImplementation = Wrap content sections in descriptive XML tags like <sales_records></sales_records> or <my_code></my_code> rather than dumping unstructured text.\n\nTag naming = Use descriptive, specific tag names (e.g., \"sales_records\" better than \"data\") to provide context about content nature.\n\nExample use case = Debugging prompt with mixed code and documentation becomes clearer when separated into <my_code> and <docs> tags.\n\nBenefits = Makes prompt structure obvious to AI, reduces confusion about content boundaries, improves output quality even for smaller content blocks.\n\nApplication = Can wrap any interpolated content like <athlete_information> even when content is short, to clarify it's external input requiring consideration.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Project Overview",
    "content": "**Project Overview**\n\nGoal = Teach Claude to set time-based reminders through tool implementation in Jupyter notebook\n\nTarget interaction = User: \"Set reminder for doctor's appointment, week from Thursday\" → Claude: \"I will remind you at that point in time\"\n\n**Three core problems requiring tools:**\n\n1. Time knowledge gap = Claude knows current date but not exact time\n2. Time calculation errors = Claude sometimes miscalculates time-based addition (e.g., 379 days from January 13th, 1973)\n3. No reminder mechanism = Claude understands reminder concept but lacks implementation capability\n\n**Three corresponding tools to build:**\n\n1. Current datetime tool = Gets current date + time\n2. Duration addition tool = Adds time duration to datetime (e.g., current date + 20 days)\n3. Reminder setting tool = Actually sets the reminder\n\nImplementation approach = One tool at a time, building toward multi-tool coordination",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Sending Tool Results",
    "content": "Tool Results = Results from executed tool functions sent back to Claude in follow-up requests.\n\nProcess: Execute tool function requested by Claude → Create tool result block → Send follow-up request with full conversation history.\n\nTool Result Block Structure:\n- tool_use_id = Matches ID from original tool use block to pair requests with results\n- content = Tool function output converted to string (usually JSON)\n- is_error = Boolean flag for function execution errors (default false)\n\nTool Use ID Purpose = Links multiple tool requests to correct results when Claude makes simultaneous tool calls. Each tool use gets unique ID, tool results must reference matching IDs.\n\nFollow-up Request Requirements:\n- Include complete message history (original user message + assistant tool use message + new user message with tool result)\n- Must include original tool schemas even if not using tools again\n- Tool result block goes in user message, not assistant message\n\nConversation Flow: User request → Claude assistant response (text + tool use blocks) → Server executes tool → User message with tool result block → Claude final response with integrated results.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Introducing Retrieval Augmented Generation",
    "content": "RAG = Retrieval Augmented Generation technique for querying large documents using language models.\n\nProblem: How to extract specific information from large documents (100-1000+ pages) using Claude without hitting context limits.\n\nOption 1 (Direct approach): Place entire document text directly into prompt.\n- Limitations: Hard token limits, decreased effectiveness with longer prompts, higher costs, slower processing\n\nOption 2 (RAG approach): Two-step process\n- Step 1: Break document into small chunks\n- Step 2: For user questions, find most relevant chunks and include only those in prompt\n\nRAG benefits: Model focuses on relevant content, scales to large/multiple documents, smaller prompts, lower costs, faster processing\n\nRAG downsides: More complexity, requires preprocessing, needs search mechanism to find relevant chunks, no guarantee chunks contain complete context, multiple chunking strategies possible (equal portions vs header-based)\n\nKey challenge: Defining relevance and optimal chunking strategy for specific use cases.\n\nRAG trades simplicity for scalability and efficiency but requires careful implementation and evaluation.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "The Full RAG Flow",
    "content": "RAG Flow = 7-step process combining text chunking, embeddings, and vector search to retrieve relevant context for LLM queries.\n\nStep 1: Text Chunking = Split source documents into separate text pieces\nStep 2: Generate Embeddings = Convert text chunks into numerical vectors using embedding models\nStep 3: Normalization = Scale vector magnitudes to 1.0 (handled automatically by embedding APIs)\nStep 4: Vector Database Storage = Store embeddings in specialized database optimized for numerical vector operations\nStep 5: Query Processing = Convert user question into embedding using same model\nStep 6: Similarity Search = Find most similar stored embeddings using cosine similarity calculation\nStep 7: Prompt Assembly = Combine user question with retrieved relevant text chunks, send to LLM\n\nKey Math Concepts:\n- Cosine Similarity = cosine of angle between vectors, returns values -1 to 1, closer to 1 means more similar\n- Cosine Distance = 1 minus cosine similarity, values closer to 0 mean higher similarity\n- Vector Database = performs similarity calculations to find closest matching embeddings\n\nProcess Flow: Pre-processing (steps 1-4) → User Query → Real-time retrieval (steps 5-7) → LLM Response",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Defining Tools with MCP",
    "content": "MCP server implementation using Python SDK creates tools through decorators rather than manual JSON schemas.\n\nMCP Python SDK = Official package that auto-generates tool JSON schemas from Python function definitions using @mcp.tool decorator.\n\nTool definition syntax = @mcp.tool(name=\"tool_name\", description=\"description\") + function with typed parameters using Field() for argument descriptions.\n\nTwo tools implemented:\n1. read_doc_contents = Takes doc_id string, returns document content from in-memory docs dictionary\n2. edit_document = Takes doc_id, old_string, new_string parameters, performs find/replace on document content\n\nError handling = Check if doc_id exists in docs dictionary, raise ValueError if not found.\n\nKey advantage = SDK eliminates manual JSON schema writing, generates schemas automatically from Python function signatures and decorators.\n\nRequired imports = Field from pydantic for parameter descriptions, mcp package for server and tool decorators.\n\nImplementation pattern = Decorator defines tool metadata, function parameters define tool arguments with types and descriptions, function body contains tool logic.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Anthropic Apps",
    "content": "Anthropic Apps = two deployed applications by Anthropic: Claude Code and Computer Use.\n\nClaude Code = terminal-based coding assistant that serves as example of agent architecture.\n\nComputer Use = toolset that expands Claude's capabilities beyond text generation.\n\nKey purpose = these apps demonstrate agent concepts and provide practical examples for understanding agent design and implementation.\n\nSetup process = involves terminal configuration for Claude Code usage on sample projects.\n\nAgent connection = both applications exemplify how agents work, serving as learning models for building effective agents.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Claude Code Setup",
    "content": "Claude Code = terminal-based coding assistant program that helps with code-related tasks\n\nCore capabilities = search/read/edit files + advanced tools (web fetching, terminal access) + MCP client support for expanded functionality via MCP servers\n\nSetup process:\n1. Install Node.js (check with \"npm help\" command)\n2. Run npm install to install Claude Code\n3. Execute \"claude\" command in terminal to login to Anthropic account\n\nFull setup guide = docs.anthropic.com\n\nMCP client functionality = can consume tools from MCP servers to extend capabilities beyond basic file operations",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Agents and Tools",
    "content": "Agents = AI systems that create plans to complete tasks using provided tools, effective when exact steps are unknown. Workflows = better when precise steps are known.\n\nKey differences: Workflows require predetermined steps, agents dynamically plan using available tools.\n\nAgent advantages: Flexibility to solve variety of tasks with same toolset, can combine tools in unexpected ways.\n\nTool abstraction principle: Provide generic/abstract tools rather than hyper-specialized ones. Example - Claude code uses bash, web_fetch, file_write (abstract) rather than refactor_tool, install_dependencies (specialized).\n\nTool combination examples: get_current_datetime + add_duration + set_reminder can solve various time-related tasks through different combinations.\n\nAgent behavior: Can request additional information when needed, combines tools creatively to achieve goals, works best with small set of flexible tools.\n\nDesign approach: Give agent abstract tools that can be pieced together rather than single-purpose specialized tools. This enables dynamic problem-solving and unexpected use cases.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Environment Inspection",
    "content": "Environment Inspection = agents evaluating their environment and action results to understand progress and handle errors.\n\nCore concept: After each action, agents need feedback mechanisms beyond basic tool returns to understand new environment state.\n\nComputer use example: Claude takes screenshot after every action (typing, clicking) to see how environment changed, since it cannot predict exact results of actions like button clicks.\n\nCode editing example: Before modifying files, agents must read current file contents to understand existing state.\n\nSocial media video agent applications:\n- Use Whisper CPP via bash to generate timestamped captions, verify dialogue placement\n- Use FFmpeg to extract video screenshots at intervals, inspect visual results\n- Validate video creation meets expectations before posting\n\nKey benefit: Environment inspection enables agents to gauge task progress, detect errors, and adapt to unexpected results rather than operating blindly.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "System Prompts",
    "content": "System Prompts = technique to customize Claude's response style and tone by assigning it a specific role or behavior pattern.\n\nImplementation = pass system prompt as plain string to create function using system keyword argument.\n\nPurpose = control how Claude responds rather than what it responds. Example: math tutor role makes Claude give hints instead of direct answers.\n\nStructure = first line typically assigns role (\"You are a patient math tutor\"), followed by specific behavioral instructions.\n\nKey principle = system prompts guide response approach, not content. Same question gets different treatment based on assigned role.\n\nTechnical implementation = create params dictionary, conditionally add system key if prompt provided, pass params to create function with ** unpacking. Handle None case by excluding system parameter entirely.\n\nUse case example = Math tutor that gives guidance/hints rather than complete solutions, encouraging student thinking over direct answers.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Response Streaming",
    "content": "Response Streaming = technique to display AI responses chunk-by-chunk as they're generated instead of waiting for complete response.\n\nProblem solved: AI responses can take 10-30 seconds. Users expect immediate feedback, not just spinners.\n\nHow it works:\n1. Server sends user message to Claude\n2. Claude immediately sends initial response (no text, just acknowledgment)\n3. Stream of events follows, each containing text chunks\n4. Server forwards chunks to frontend for real-time display\n\nEvent types:\n- message_start = initial acknowledgment\n- content_block_start = text generation begins\n- content_block_delta = contains actual text chunks (most important)\n- content_block_stop/message_stop = generation complete\n\nImplementation:\nBasic: client.messages.create(stream=True) returns event iterator\nSimplified: client.messages.stream() with text_stream property extracts just text\nFinal message: stream.get_final_message() assembles all chunks for storage\n\nKey benefits: Better UX through immediate response visibility, complete message capture for database storage.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Model Based Grading",
    "content": "Model Based Grading = evaluation system that takes model outputs and assigns objective scores (typically 1-10 scale, 10 = highest quality)\n\nThree grader types:\n- Code graders = programmatic checks (length, word presence, syntax validation, readability scores)\n- Model graders = additional API call to evaluate original model output, highly flexible for quality/instruction-following assessment\n- Human graders = person evaluates responses, most flexible but time-consuming and tedious\n\nKey requirements: Must return objective signal (usually numerical score). Define evaluation criteria upfront.\n\nImplementation pattern for model graders:\n- Create detailed prompt requesting strengths/weaknesses/reasoning/score (not just score alone to avoid default middling scores)\n- Use JSON response format with pre-filled assistant message and stop sequences\n- Parse returned JSON for score and reasoning\n- Calculate average scores across test cases for final metric\n\nModel graders offer high flexibility but may be inconsistent. Still provides objective baseline for prompt optimization.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Code Based Grading",
    "content": "Code Based Grading = automated validation system for LLM outputs containing code, JSON, or regex\n\nCore Implementation:\n- validate_json() = attempts JSON parsing, returns 10 if valid, 0 if error\n- validate_python() = attempts AST parsing, returns 10 if valid, 0 if error  \n- validate_regex() = attempts regex compilation, returns 10 if valid, 0 if error\n\nDataset Requirements:\n- Must include \"format\" key specifying expected output type (JSON/Python/RegEx)\n- Updated via prompt template modification for automated dataset generation\n\nPrompt Engineering:\n- Instruct model to respond only with raw code/JSON/regex\n- No comments, explanations, or commentary\n- Use pre-filled Assistant message with \\`\\`\\`code\\`\\`\\` blocks\n- Add stop sequences to extract clean output\n\nScoring System:\n- Final score = (model_score + syntax_score) / 2\n- Combines semantic evaluation with syntax validation\n- Enables measurement of both correctness and technical validity\n\nKey Limitation = requires known expected format for proper validator selection",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "The Batch Tool",
    "content": "Batch Tool = tool that enables Claude to run multiple tools in parallel within a single Assistant message instead of making separate sequential requests.\n\nProblem: Claude can technically send multiple tool use blocks in one message but rarely does so in practice, leading to unnecessary sequential tool calls.\n\nSolution: Create batch tool schema that takes list of invocations (each containing tool name + arguments). Instead of calling tools directly, Claude calls batch tool with array of desired tool executions.\n\nImplementation:\n- Add batch tool to schema with invocations parameter\n- Create run_batch function that iterates through invocations list\n- Extract tool name and JSON-parsed arguments from each invocation\n- Call run_tool function for each requested tool\n- Return batch_output list containing results from all tool executions\n\nMechanism: Tricks Claude into parallel tool execution by providing higher-level abstraction that manually handles what multiple tool use blocks would accomplish automatically.\n\nResult: Single request-response cycle instead of multiple sequential rounds for parallel-executable tasks.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "BM25 Lexical Search",
    "content": "BM25 = Best Match 25, a lexical search algorithm commonly used in RAG pipelines to complement semantic search.\n\nProblem with semantic search alone = Can miss exact term matches, returning irrelevant results even when specific terms appear frequently in certain documents.\n\nHybrid search approach = Combines semantic search (embeddings/vector database) with lexical search (BM25) in parallel, then merges results for better balance.\n\nBM25 algorithm steps:\n1. Tokenize user query into separate terms (remove punctuation, split on spaces)\n2. Count frequency of each term across all text chunks/documents\n3. Assign relative importance to terms based on usage frequency (rare terms = higher importance, common terms like \"a\" = lower importance)\n4. Rank text chunks by how often they contain higher-weighted terms\n\nKey insight = Frequently used terms across corpus are less important for search relevance than rare, specific terms.\n\nBM25 advantages = Better at finding exact term matches, prioritizes documents containing rare/specific search terms, complements semantic search weaknesses.\n\nImplementation = Both semantic and lexical search systems use similar APIs (add_document, search functions) making them easy to combine.\n\nNext step = Merge results from both search systems to get benefits of semantic understanding plus exact term matching.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Reranking Results",
    "content": "Reranking = post-processing step that uses LLM to reorder search results by relevance after initial retrieval.\n\nProcess: Run vector + BM25 search → merge results → pass to LLM with prompt asking to rank documents by relevance → get reordered results.\n\nImplementation details: Use document IDs instead of full text for efficiency. LLM receives user query + candidate documents + instruction to return most relevant docs in decreasing order. Assistant message pre-fill + stop sequence ensures structured JSON output.\n\nTradeoffs: Increases search accuracy by leveraging LLM's understanding of semantic relevance. Increases latency due to additional LLM call. Particularly effective when initial retrieval methods miss nuanced query intent (e.g., \"ENG team\" vs \"engineering team\").\n\nExample improvement: Query \"What did engineering team do with incident 2023?\" correctly prioritized software engineering section over cybersecurity section after reranking, despite hybrid search initially ranking it lower.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Code Execution and the Files API",
    "content": "Files API = allows uploading files ahead of time and referencing them later via file ID instead of including raw file data in each request. Upload file → get file metadata object with ID → use ID in future requests.\n\nCode Execution = server-based tool where Claude executes Python code in isolated Docker containers. No implementation needed, just include predefined tool schema. Claude can run code multiple times, interpret results, generate final response.\n\nKey constraints: Docker containers have no network access. Data input/output relies on Files API integration.\n\nCombined workflow: Upload file via Files API → get file ID → include ID in container upload block → ask Claude to analyze → Claude writes/executes code with access to uploaded file → returns analysis and results.\n\nClaude can generate files (plots, reports) inside container that can be downloaded using file IDs returned in response.\n\nUse cases: Data analysis, file processing, automated code generation for complex tasks. Response contains code blocks, execution results, and final analysis.\n\nImplementation: Use container upload block with file ID, include analysis prompt, Claude handles code execution automatically.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "The Server Inspector",
    "content": "MCP Inspector = in-browser debugger for testing MCP servers without connecting to applications\n\nAccess: Run \\`mcp dev [server_file.py]\\` in terminal → opens server on port → navigate to provided URL in browser\n\nInterface: Left sidebar has connect button → top menu shows resources/prompts/tools sections → tools section lists available tools → click tool to open right panel for manual testing\n\nTesting workflow: Connect to server → navigate to tools → select specific tool → input required parameters → click run tool → verify output\n\nKey features: Live development testing, manual tool invocation, parameter input forms, success/failure feedback, no need for full application integration\n\nNote: UI actively changing during development, core functionality remains similar\n\nExample usage: Test document tools by inputting document IDs, verify read operations, test edit operations, chain operations to verify changes\n\nPrimary benefit: Debug MCP server implementations efficiently during development phase",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Implementing a Client",
    "content": "MCP Client Implementation:\n\nMCP Client = wrapper class around client session for resource cleanup and connection management to MCP server\n\nClient Session = actual connection to MCP server from MCP Python SDK, requires resource cleanup on close\n\nClient Purpose = exposes MCP server functionality to rest of codebase, enables reaching out to server for tool lists and tool execution\n\nKey Functions:\n- list_tools() = await self.session.list_tools(), return result.tools\n- call_tool() = await self.session.call_tool(tool_name, tool_input)\n\nUsage Flow = client gets tool definitions to send to Claude, then executes tools when Claude requests them\n\nCommon Pattern = wrap client session in larger class for resource management rather than use session directly\n\nTesting = can run client file directly with testing harness to verify server connection and tool retrieval\n\nIntegration = other code in project calls client functions to interact with MCP server, enabling Claude to inspect/edit documents through defined tools",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Parallelizing Claude Code",
    "content": "Parallelizing Claude Code = running multiple Claude instances simultaneously to complete different tasks in parallel\n\nCore Problem = multiple Claude instances modifying same files simultaneously creates conflicts and invalid code\n\nSolution = Git work trees providing isolated workspaces per Claude instance\n\nGit Work Trees = feature creating complete project copies in separate directories, each corresponding to different Git branches\n\nWorkflow = create work tree → assign task to Claude instance → work in isolation → commit changes → merge back to main branch\n\nCustom Commands = automating work tree creation/management through .claude/commands directory with markdown files containing prompts\n\nCommand Structure = .claude/commands/filename.md with $ARGUMENTS placeholder for dynamic values\n\nParallel Execution Benefits = single developer commanding virtual team of software engineers, major productivity scaling limited only by engineer's management capacity\n\nMerge Conflicts = Claude automatically resolves conflicts during branch merging process\n\nCleanup = Claude handles work tree removal after feature completion\n\nKey Advantage = scales to unlimited parallel instances based on developer's capacity to manage simultaneous tasks",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "How Computer Use Works",
    "content": "Computer use = tool system implementation allowing Claude to interact with computing environments\n\nTool use flow: User sends message + tool schema → Claude responds with tool use request (ID, name, input) → Server executes code → Result sent back to Claude as tool result\n\nComputer use follows identical flow:\n- Special tool schema sent to Claude (small schema expands to larger structure behind scenes)\n- Expanded schema includes action function with arguments: mouse move, left click, screenshot, etc.\n- Claude sends tool use request\n- Developers must fulfill request via computing environment (typically Docker container)\n- Container executes programmatic key presses/mouse movements\n- Response sent back to Claude\n\nKey points:\n- Claude doesn't directly manipulate computers\n- Computer use = tool system + developer-provided computing environment\n- Anthropic provides reference implementation (Docker container with pre-built mouse/keyboard execution code)\n- Setup requires Docker + simple command execution\n- Enables direct chat interface for testing Claude's computer use functionality\n\nComputer use = abstraction layer where tool system handles Claude communication while Docker container handles actual computer interactions.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Parallelization Workflows",
    "content": "Parallelization Workflows = breaking one complex task into multiple simultaneous subtasks, then aggregating results.\n\nExample: Material selection for parts\n- Instead of: One large prompt asking Claude to choose between metal/polymer/ceramic/composite with all criteria\n- Use: Separate parallel requests, each evaluating one material's suitability, then final aggregation step to compare results\n\nStructure: Input → Multiple parallel subtasks → Aggregator → Final output\n\nBenefits:\n- Focus = Each subtask handles one specific analysis instead of juggling multiple considerations\n- Modularity = Individual prompts can be improved/evaluated separately  \n- Scalability = Easy to add new subtasks without affecting existing ones\n- Quality = Reduces confusion from overly complex single prompts\n\nKey principle: Decompose complex decisions into specialized parallel analyses, then synthesize results.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Overview of Claude Models",
    "content": "Claude has three model families optimized for different priorities:\n\nOpus = highest intelligence model for complex, multi-step tasks requiring deep reasoning and planning. Trade-off: higher cost and latency.\n\nSonnet = balanced model with good intelligence, speed, and cost efficiency. Strong coding abilities and precise code editing. Best for most practical use cases.\n\nHaiku = fastest model optimized for speed and cost efficiency. No reasoning capabilities like Opus/Sonnet. Best for real-time user interactions and high-volume processing.\n\nSelection framework: Intelligence priority → Opus. Speed priority → Haiku. Balanced requirements → Sonnet.\n\nCommon approach = use multiple models in same application based on specific task requirements rather than single model selection.\n\nAll models share core capabilities: text generation, coding, image analysis. Main difference is optimization focus.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Generating Test Datasets",
    "content": "Custom prompt evaluation workflow = build prompt + generate test dataset + evaluate performance\n\nGoal = AWS code assistance prompt that outputs only Python, JSON config, or regex without explanations\n\nDataset generation approaches = manual assembly or automated with Claude (use faster models like Haiku for generation)\n\nDataset structure = array of JSON objects with task property describing user requests\n\nGeneration process = prompt Claude to create test cases → use pre-filling with assistant message \"\\`\\`\\`json\" → set stop sequence \"\\`\\`\\`\" → parse response as JSON → save to file\n\nKey implementation = generate_dataset() function that sends prompt to Claude, gets structured JSON response of test tasks, saves to dataset.json file for later evaluation use\n\nTest dataset enables systematic evaluation by running prompt against multiple input scenarios to measure performance consistency.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Providing Examples",
    "content": "One-shot/Multi-shot prompting = providing examples in prompts to guide model behavior. One-shot = single example, multi-shot = multiple examples.\n\nImplementation: Structure examples with XML tags containing sample input and ideal output. Always wrap examples clearly to distinguish from actual prompt content.\n\nKey applications:\n- Corner case handling (sarcasm detection, edge scenarios)\n- Complex output formatting (JSON structures, specific formats)\n- Clarifying expected response quality/style\n\nBest practices:\n- Add context for corner cases (\"be especially careful with sarcasm\")\n- Include reasoning explaining why output is ideal\n- Use highest-scoring examples from prompt evaluations as templates\n- Place examples after main instructions/guidelines\n\nEffectiveness boost: Combine examples with explanations of what makes them ideal to reinforce desired output characteristics.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "PDF Support",
    "content": "PDF Support in Claude:\n\nClaude can read PDF files directly using similar code to image processing. \n\nKey implementation changes:\n- File type = \"document\" instead of \"image\"\n- Media type = \"application/pdf\" instead of \"image/png\"\n- Variable naming = file_bytes instead of image_bytes\n\nClaude PDF capabilities = read text + images + charts + tables + mixed content extraction\n\nPDF processing = one-stop solution for comprehensive document analysis\n\nUsage pattern = same as image input but with document-specific parameters",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Defining Resources",
    "content": "MCP Resources = mechanism allowing MCP servers to expose data to clients for read operations\n\nResource Types = 2 types: direct (static URI like \"docs://documents\") and templated (parameterized URI like \"docs://documents/{doc_id}\")\n\nURI = address/identifier for accessing specific resource, defined when creating resource\n\nResource Flow = client sends read resource request with URI → server matches URI to function → server executes function → returns data in read resource result\n\nImplementation = use @mcp.resource decorator with URI and MIME type parameters\n\nMIME Types = hint to client about returned data format (application/json for structured data, text/plain for plain text)\n\nTemplated Resources = URI parameters automatically parsed by SDK and passed as keyword arguments to handler function\n\nResource vs Tools = resources provide data proactively (fetch document contents when @ mentioned), tools perform actions reactively (when Claude decides to call them)\n\nData Return = SDK automatically serializes returned data to strings, client responsible for deserialization\n\nTesting = MCP inspector can list direct resources separately from templated resources, allows testing individual resource calls",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Accessing Resources",
    "content": "MCP Resource Access Implementation:\n\nResource Reading Function = client-side function to request and parse resources from MCP server\n\nFunction Parameters = URI (resource identifier)\n\nImplementation Steps:\n- Import json module + AnyURL from pydantic\n- Call await self.session.read_resource(AnyURL(uri))\n- Extract first element from result.contents[0]\n- Check resource.mime_type for parsing strategy\n\nContent Parsing Logic:\n- If mime_type == \"application/json\" → return json.loads(resource.text)\n- Otherwise → return resource.text (plain text)\n\nServer Response Structure = result.contents list with first element containing type/mime_type metadata\n\nResource Integration = MCP client functions called by other application components to fetch document contents for prompts\n\nEnd Result = Document contents automatically included in Claude prompts without requiring tool calls\n\nKey Point = Resources expose server information directly to clients through structured request/response pattern",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Prompt Evaluation",
    "content": "Prompt Engineering = techniques for writing/editing prompts to help Claude understand requests and desired responses.\n\nPrompt Evaluation = automated testing of prompts using objective metrics to measure effectiveness.\n\nThree paths after writing a prompt:\n1. Test once/twice, deploy to production (trap)\n2. Test with custom inputs, minor tweaks for corner cases (trap)  \n3. Run through evaluation pipeline for objective scoring (recommended)\n\nKey takeaway: Engineers commonly under-test prompts. Use evaluation pipelines to get objective performance scores before iterating and deploying prompts.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Rules of Prompt Caching",
    "content": "Prompt Caching = system that saves processing work from initial request to reuse in follow-up requests with identical content\n\nCore mechanism: Initial request → Claude processes + saves work to cache → Follow-up requests with identical content → Claude retrieves cached work instead of reprocessing\n\nCache duration = 1 hour maximum\n\nCache activation requires manual cache breakpoint addition to message blocks\n\nText block formats:\n- Shorthand: content = \"text string\" (cannot add cache control)\n- Longhand: content = [{\"type\": \"text\", \"text\": \"content\", \"cache_control\": {...}}] (required for caching)\n\nCache scope = all content up to and including breakpoint gets cached\n\nCache invalidation = any change in content before breakpoint invalidates entire cache\n\nContent processing order = tools → system prompt → messages (joined together)\n\nCache breakpoint placement options:\n- Tool schemas\n- System prompts  \n- Message blocks (text, image, tool use, tool result)\n\nMaximum breakpoints = 4 per request\n\nMultiple breakpoints = create multiple cache layers, partial cache hits possible if only later content changes\n\nMinimum cache threshold = 1024 tokens required for content to be cached\n\nBest use cases = repeated identical content (system prompts, tool definitions, static message prefixes)",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Citations",
    "content": "Citations = feature allowing Claude to reference source documents and show where information comes from\n\nCitation types:\n- citation_page_location = for PDF documents, shows document index/title/start page/end page/cited text\n- citation_char_location = for plain text, shows character position in text block\n\nImplementation:\n- Add \"citations\": {\"enabled\": true} to request\n- Add \"title\" field to identify source document\n- Works with both PDF files and plain text sources\n\nResponse structure = content becomes list of text blocks, some containing citations arrays with location data\n\nPurpose = transparency for users to verify Claude's information sources and check accuracy of interpretations\n\nUI benefit = enables citation popups/overlays showing source document, page numbers, and exact cited text when users hover over referenced content\n\nKey use case = ensuring users can investigate how Claude builds responses from source materials rather than appearing to speak from memory alone",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  },
  {
    "title": "Workflows vs Agents",
    "content": "Workflows = pre-defined series of calls to Claude with known exact steps. Agents = flexible approach using basic tools that Claude combines to complete unknown tasks.\n\nKey differences:\n\nTask division: Workflows break big tasks into smaller, specific subtasks enabling higher focus and accuracy. Agents handle varied challenges creatively without predetermined steps.\n\nTesting/evaluation: Workflows easier to test due to known execution sequence. Agents harder to test since execution path unpredictable.\n\nUser experience: Workflows require specific inputs. Agents create own inputs from user queries and can request additional input when needed.\n\nSuccess rates: Workflows = higher task completion rates due to structured approach. Agents = lower completion rates due to delegated complexity.\n\nRecommendation: Prioritize workflows for reliability. Use agents only when flexibility truly required. Users want 100% working products over fancy agents.\n\nCore principle: Solve problems reliably first, innovation second.",
    "lessonTitle": "Building with the Claude API",
    "lessonId": "287722"
  }
]